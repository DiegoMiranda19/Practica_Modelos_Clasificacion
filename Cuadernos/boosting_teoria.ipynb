{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; font-size: 1.2em;\">\n",
    "    <img src=\"logo-uec.png\" alt=\"Logo Universidad\" style=\"width: 150px; float: right; margin-left: 20px;\"/>\n",
    "    <b>Maestr칤a en Inteligencia de Negocios</b>\n",
    "    <br>\n",
    "    <b>Curso: T칩picos de Machine Learning y Redes Neuronales</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesi칩n 2: La Revoluci칩n del Boosting 游"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducci칩n\n",
    "\n",
    "En nuestra sesi칩n anterior, exploramos el poder de los **m칠todos de ensamble** con un enfoque en **Bagging** y su implementaci칩n m치s famosa: **Random Forest**. Vimos c칩mo la \"sabidur칤a de la multitud\", al promediar las predicciones de muchos 치rboles (entrenados en paralelo en diferentes subconjuntos de datos), puede reducir dr치sticamente la varianza y mejorar la robustez del modelo.\n",
    "\n",
    "Hoy, vamos a explorar una filosof칤a de ensamble diferente pero incre칤blemente poderosa: **Boosting**.\n",
    "\n",
    "Si Random Forest es una democracia (donde cada 치rbol vota), Boosting es una jerarqu칤a de expertos donde cada nuevo experto aprende de los errores del anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Boosting: Aprendiendo de los Errores\n",
    "\n",
    "La idea fundamental del Boosting es entrenar modelos **secuencialmente**. Cada nuevo modelo se enfoca en corregir los errores cometidos por la secuencia de modelos anteriores.\n",
    "\n",
    "A diferencia de Bagging (Random Forest) donde los 치rboles se construyen de forma independiente y en paralelo, en Boosting el proceso es **secuencial y adaptativo**.\n",
    "\n",
    "--- \n",
    "\n",
    "### Una Analog칤a Simple\n",
    "\n",
    "Imagina que eres un estudiante prepar치ndote para un examen dif칤cil:\n",
    "\n",
    "1.  **Intento 1 (Modelo 1):** Haces un examen de pr치ctica. Obtienes un 70%. Algunas preguntas te parecieron f치ciles (las acertaste) y otras muy dif칤ciles (las fallaste).\n",
    "2.  **Aprendizaje (Error):** Recibes tu calificaci칩n. Ahora sabes exactamente cu치les son tus puntos d칠biles (las preguntas que fallaste).\n",
    "3.  **Intento 2 (Modelo 2):** En lugar de volver a estudiar *todo* el material, te enfocas intensamente en los temas de las preguntas que fallaste. Haces un nuevo examen de pr치ctica sobre *esos* temas.\n",
    "4.  **Conocimiento Final (Ensamble):** Tu conocimiento final no es solo tu 칰ltimo examen, sino una combinaci칩n ponderada de tu conocimiento del primer intento y tu conocimiento enfocado del segundo intento.\n",
    "\n",
    "--- \n",
    "\n",
    "### Contraste: Bagging vs. Boosting\n",
    "\n",
    "| Caracter칤stica | Bagging (Ej. Random Forest) | Boosting (Ej. Gradient Boosting) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Proceso** | Paralelo (츼rboles independientes) | Secuencial (Un 치rbol aprende del anterior) |\n",
    "| **Objetivo Principal** | Reducir Varianza | Reducir Sesgo (y luego varianza) |\n",
    "| **Manejo de Errores** | Promedia los errores | Se enfoca en los errores y los corrige |\n",
    "| **Ponderaci칩n** | Todos los modelos votan por igual | Los modelos se ponderan (algunos tienen m치s peso) |\n",
    "| **Riesgo** | Generalmente robusto al overfitting | Puede sobreajustar si se usan demasiados 치rboles |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. El Algoritmo de Boosting y la Tasa de Aprendizaje\n",
    "\n",
    "El algoritmo que define el \"Gradient Boosting\" moderno se populariz칩 en el art칤culo \"Greedy Function Approximation: A Gradient Boosting Machine\" (Friedman, 2001) y es el que se muestra en la imagen que compartiste (Algoritmo 8.2 de Hastie & Tibshirani).\n",
    "\n",
    "Vamos a desglosarlo:\n",
    "\n",
    "### El Algoritmo Base (Visto en Hastie & Tibshirani)\n",
    "\n",
    "1.  **Inicializar:** Se fija la predicci칩n inicial `f(x) = 0`. El \"error\" o **residuo** inicial (`r_i`) es simplemente el valor real que queremos predecir: `r_i = y_i`.\n",
    "\n",
    "2.  **Iterar** para `b = 1` hasta `B` (donde `B` es el n칰mero total de 치rboles que queremos entrenar):\n",
    "    \n",
    "    a.  Entrenar un 치rbol de regresi칩n `h^b(x)` para que prediga los **residuos actuales** `r_i` (usando `X` como variables predictoras y `r` como variable objetivo).\n",
    "    \n",
    "    b.  Actualizar la predicci칩n total `f(x)` a침adiendo una *versi칩n \"encogida\"* del nuevo 치rbol:\n",
    "        `f(x) <-- f(x) + 풭 * h^b(x)`\n",
    "        \n",
    "    c.  Actualizar los residuos para la *siguiente* iteraci칩n (el siguiente 치rbol):\n",
    "        `r_i <-- r_i - 풭 * h^b(x)`\n",
    "\n",
    "3.  **Resultado Final:** El modelo final `F(x)` es la suma de todas las contribuciones de los 치rboles: `F(x) = 풖 [풭 * h^b(x)]`\n",
    "\n",
    "--- \n",
    "\n",
    "### 쯈u칠 es la Tasa de Aprendizaje (`풭`)?\n",
    "\n",
    "El par치metro `풭` (lambda) se conoce como la **tasa de aprendizaje** (*learning rate*) o *encogimiento* (*shrinkage*). Es uno de los hiperpar치metros m치s importantes del modelo.\n",
    "\n",
    "* **쯈u칠 hace?** Controla *cu치nto* del aprendizaje del nuevo 치rbol (`h^b(x)`) a침adimos a la predicci칩n total en cada paso.\n",
    "* **쯇or qu칠?** Es la principal herramienta para **prevenir el overfitting**. Si `풭 = 1`, cada nuevo 치rbol intenta corregir *todo* el error restante, lo cual es muy agresivo y lleva a que el modelo memorice el ruido de los datos de entrenamiento.\n",
    "* **La Estrategia:** Es mejor \"dar pasos peque침os y seguros\". Al usar un `풭` peque침o (ej. 0.1, 0.05, o 0.01), cada 치rbol solo corrige una peque침a fracci칩n del error. Esto hace que el aprendizaje sea m치s lento, pero mucho m치s robusto y generalizable.\n",
    "\n",
    "**La compensaci칩n (Trade-off):**\n",
    "* Un `풭` **peque침o** requiere un `B` (n칰mero de 치rboles) **grande** para alcanzar un buen rendimiento.\n",
    "* Un `풭` **grande** aprende r치pido (requiere menos 치rboles), pero es muy probable que sobreajuste.\n",
    "\n",
    "--- \n",
    "\n",
    "### Ejemplo Intuitivo (con Tasa de Aprendizaje)\n",
    "\n",
    "Volvamos a nuestro ejemplo de predecir precios de casas, pero ahora con `풭 = 0.1`.\n",
    "\n",
    "**Paso 1: El Modelo Base (츼rbol 1)**\n",
    "Predicci칩n promedio: **$300k**.\n",
    "Residuo (Casa A): $350k (Real) - $300k (Pred) = **+$50k**.\n",
    "\n",
    "**Paso 2: El Segundo Modelo (츼rbol 2)**\n",
    "El 츼rbol 2 se entrena con los residuos. Supongamos que para la Casa A, predice un residuo de **+$40k**.\n",
    "\n",
    "**Paso 3: Combinar las Predicciones (con `풭 = 0.1`)**\n",
    "\n",
    "`Predicci칩n Actualizada = Predicci칩n Anterior + 풭 * Predicci칩n 츼rbol 2`\n",
    "`Predicci칩n Actualizada = $300k + 0.1 * $40k`\n",
    "`Predicci칩n Actualizada = $300k + $4k = $304k`\n",
    "\n",
    "Nuestra nueva predicci칩n es **$304k**. N칩tese que no saltamos directamente a $340k. Nos movemos solo un 10% del camino.\n",
    "\n",
    "**Paso 4: Repetir (Iterar)**\n",
    "\n",
    "Ahora, calculamos un *nuevo* residuo para el siguiente 치rbol:\n",
    "\n",
    "`Nuevo Residuo = Valor Real - Predicci칩n Actualizada`\n",
    "`Nuevo Residuo = $350k - $304k = +$46k`\n",
    "\n",
    "El **츼rbol 3** se entrenar치 para predecir este nuevo residuo de **+$46k**. El proceso se repite, refinando lentamente la predicci칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 쯏 AdaBoost? 쮺u치l es la Diferencia?\n",
    "\n",
    "Es com칰n confundir AdaBoost (Adaptive Boosting) con Gradient Boosting, pero funcionan de manera fundamentalmente diferente.\n",
    "\n",
    "**AdaBoost (Adaptive Boosting):**\n",
    "\n",
    "1.  **Ponderaci칩n de Muestras:** AdaBoost comienza dando a todas las muestras (filas) de datos el mismo peso o importancia.\n",
    "2.  **Entrenar Modelo Simple:** Entrena un modelo simple (usualmente un \"치rbol toc칩n\" o *stump*, un 치rbol con una sola divisi칩n).\n",
    "3.  **Identificar Errores:** Mira qu칠 muestras clasific칩 *mal*.\n",
    "4.  **Adaptar (Adaptive):** *Aumenta el peso* (la importancia) de esas muestras mal clasificadas. Las muestras que se clasificaron bien ven su peso reducido.\n",
    "5.  **Entrenar Siguiente Modelo:** El segundo modelo se entrena, pero ahora tiene una *penalizaci칩n extra* por fallar en las muestras que el primer modelo ya fall칩 (porque ahora \"valen m치s\").\n",
    "\n",
    "**Diferencia Clave:**\n",
    "\n",
    "| | **Gradient Boosting (GBM)** | **AdaBoost** |\n",
    "| :--- | :--- | :--- |\n",
    "| **En qu칠 se enfoca** | En el **error de predicci칩n (Residuo)**. | En las **muestras (filas)** mal clasificadas. |\n",
    "| **C칩mo aprende** | El siguiente modelo predice los *residuos* del modelo anterior. | El siguiente modelo da m치s importancia a las *muestras* que el modelo anterior fall칩. |\n",
    "| **Variable Objetivo** | La \"y\" cambia en cada paso (pasa a ser el residuo). | La \"y\" siempre es la misma, pero los *pesos de las muestras* cambian. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 쮻칩nde entra el \"Gradiente\" en Gradient Boosting?\n",
    "\n",
    "En un modelo de regresi칩n lineal (OLS), 쯖칩mo encontramos los mejores coeficientes? Minimizando una **funci칩n de p칠rdida**, la Suma de Errores al Cuadrado (SEC): `풖(y_real - y_pred)`.\n",
    "\n",
    "쯏 c칩mo minimizamos cualquier funci칩n en c치lculo? Encontrando d칩nde su **gradiente** (la derivada) es cero. En Machine Learning, usamos un algoritmo llamado **Gradiente Descendente**: calculamos el gradiente y nos movemos en la direcci칩n opuesta (el *gradiente negativo*) para reducir el error. \n",
    "\n",
    "### 춰Los \"Residuos\" son un Gradiente!\n",
    "\n",
    "La genialidad de Friedman (el creador de GBM) fue darse cuenta de esto:\n",
    "\n",
    "> **Para la funci칩n de p칠rdida de Error Cuadr치tico (SEC), el *gradiente negativo* es exactamente el residuo (`y_real - y_pred`).**\n",
    "\n",
    "El algoritmo que vimos en la Secci칩n 2 (entrenar 치rboles sobre los residuos) es, de hecho, una forma de **Gradiente Descendente** en el \"espacio de las funciones\". Cada nuevo 치rbol `h(x)` es un peque침o paso en la direcci칩n que m치s reduce el error.\n",
    "\n",
    "### 쯇or qu칠 llamarlo \"Gradient\" Boosting?\n",
    "\n",
    "Porque esto **generaliza** el algoritmo. Ya no estamos limitados a usar residuos (que solo funcionan para el Error Cuadr치tico).\n",
    "\n",
    "Podemos usar **cualquier funci칩n de p칠rdida que sea derivable**:\n",
    "\n",
    "* **Clasificaci칩n:** Usamos *Log-Loss* (P칠rdida Logar칤tmica).\n",
    "* **Regresi칩n Robusta:** Usamos *MAE* (Error Absoluto Medio) o *Huber Loss*.\n",
    "\n",
    "En cada paso, el nuevo 치rbol no se entrena para predecir el residuo, sino para predecir el **gradiente negativo de la funci칩n de p칠rdida** de cada muestra. Esto es lo que hace que GBM sea una navaja suiza para casi cualquier problema de ML supervisado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Los Campeones de la Industria: XGBoost y LightGBM\n",
    "\n",
    "Si GBM es tan bueno, 쯣or qu칠 todos usan **XGBoost** y **LightGBM**? Porque toman esta idea del gradiente y la optimizan a un nivel extremo (de ah칤 el nombre \"eXtreme Gradient Boosting\").\n",
    "\n",
    "--- \n",
    "\n",
    "### XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "**Mejoras Clave:**\n",
    "\n",
    "1.  **La Mejora del Gradiente (La conexi칩n):** XGBoost lleva la idea del gradiente un paso m치s all치. \n",
    "    * GBM cl치sico usa solo el **gradiente** (la \"primera derivada\" de la funci칩n de p칠rdida). \n",
    "    * XGBoost usa *ambos*, el **gradiente** (1춹 derivada) y el **Hessiano** (la \"segunda derivada\").\n",
    "    * **Intuici칩n:** Si el gradiente (1춹 derivada) te dice *en qu칠 direcci칩n* moverte para reducir el error (la *pendiente*), el Hessiano (2춹 derivada) te dice *cu치nto* debes moverte para encontrar el m칤nimo (la *curvatura*). Usar ambos es mucho m치s r치pido y preciso, como un m칠todo de Newton-Raphson.\n",
    "\n",
    "2.  **Regularizaci칩n (L1 y L2):** Esta es su arma secreta. Al igual que en la Regresi칩n Lineal (Ridge y Lasso), XGBoost a침ade una penalizaci칩n a la complejidad del modelo (profundidad, n칰mero de hojas), previniendo el sobreajuste por dise침o.\n",
    "\n",
    "3.  **Optimizaci칩n de Velocidad (Paralelismo):** Paraleliza la *construcci칩n de cada 치rbol* (evaluando cortes en paralelo).\n",
    "\n",
    "4.  **Manejo de Valores Nulos (Missing Values):** Aprende la mejor \"ruta\" para los valores nulos durante el entrenamiento.\n",
    "\n",
    "--- \n",
    "\n",
    "### LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "Desarrollado por Microsoft, es (generalmente) a칰n m치s r치pido que XGBoost.\n",
    "\n",
    "**Mejoras Clave:**\n",
    "\n",
    "1.  **Misma base de Gradiente/Hessiano:** Usa la misma optimizaci칩n de 1춹 y 2춹 derivada que XGBoost.\n",
    "\n",
    "2.  **Velocidad Extrema (Crecimiento *Leaf-wise*):**\n",
    "    * *GBM/XGBoost (Level-wise):* Construyen el 치rbol nivel por nivel, de forma sim칠trica.\n",
    "    * *LightGBM (Leaf-wise):* Crece el 치rbol de forma asim칠trica. Identifica la hoja que *m치s reducir치 la p칠rdida* y la expande. Es m치s r치pido y preciso, pero puede sobreajustar en datasets peque침os.\n",
    "\n",
    "3.  **Manejo Eficiente de Variables Categ칩ricas:**\n",
    "    * Puede manejarlas de forma nativa sin necesidad de One-Hot Encoding, ahorrando much칤sima memoria y tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusi칩n de la Teor칤a\n",
    "\n",
    "Hemos pasado del concepto de \"aprender de los errores\" (Boosting) al algoritmo de \"predecir los residuos\" (Boosting Cl치sico), entendido por qu칠 se llama \"**Gradient**\" Boosting (es un descenso de gradiente sobre una funci칩n de p칠rdida) y finalmente visto c칩mo **XGBoost/LightGBM** lo perfeccionan usando derivadas de segundo orden (el Hessiano).\n",
    "\n",
    "Ahora, 춰vamos al laboratorio a aplicar esto!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
